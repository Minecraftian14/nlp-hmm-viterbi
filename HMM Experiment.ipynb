{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e555f9bd",
   "metadata": {},
   "source": [
    "# Accuracy Evaluation Script of Hidden Markov Model for Part-of-Speech tagging task\n",
    "\n",
    "## Team Details\n",
    "\n",
    "|               |           |                               |\n",
    "|---------------|-----------|-------------------------------|\n",
    "| Subject       |           | Natural Language Processing   |\n",
    "| Supervisor    |           | Prof. Tulika Saha             |\n",
    "| Group No.     |           | 6                             |\n",
    "| Member 1      | Name:     | Shreya Gupta                  |\n",
    "|               | Roll No:  | MT2025724                     |\n",
    "| Member 2      | Name:     | Anirudh Sharma                |\n",
    "|               | Roll No:  | MT2025732                     |\n",
    "| Member 3      | Name:     | Saatvik Sinha                 |\n",
    "|               | Roll No:  | MT2025722                     |\n",
    "| Member 4      | Name:     | Varshith M Gowda              |\n",
    "|               | Roll No:  | BT2024227                     |\n",
    "\n",
    "## Overall Summary\n",
    "\n",
    "| Metrics of the Report             |                   |\n",
    "|-----------------------------------|-------------------|\n",
    "| Training Dataset Size             | 12544 sentences   |\n",
    "| Test Dataset Size                 | 2077 sentences    |\n",
    "| No. of Unique Words               | 9075              |\n",
    "| No. of POS Tags                   | 17                |\n",
    "| Micro Accuracy on Test Dataset    | 89.49%            |\n",
    "| Macro Accuracy on Test Dataset    | 83.39%            |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9ef5ed8",
   "metadata": {},
   "source": [
    "## System Imports\n",
    "\n",
    "Setup code and data file paths for use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "01437bdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enabling access to files within src folder\n",
    "\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "sys.path.append(str(Path.cwd() / \"src\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Importing necessary modules\n",
    "\n",
    "from conllu_parser import parse_conllu_file\n",
    "from hmm_viterbi import HiddenMarkovModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d29ae5e182848ec8",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "95bf95986a9eee4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to the dataset file\n",
    "trainpath = \"data/UD_English-EWT/en_ewt-ud-train.conllu\"\n",
    "testpath = \"data/UD_English-EWT/en_ewt-ud-test.conllu\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d19b2b6540508fd2",
   "metadata": {},
   "source": [
    "## Load Training Data\n",
    "\n",
    "Parse the training corpus to extract sentences and POS tags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfd7289bc8d6c28e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 12544 sentences from training data\n",
      "Example: [('Al', 'PROPN'), ('-', 'PUNCT'), ('Zaman', 'PROPN'), (':', 'PUNCT'), ('American', 'ADJ'), ('forces', 'NOUN'), ('killed', 'VERB'), ('Shaikh', 'PROPN'), ('Abdullah', 'PROPN'), ('al', 'PROPN'), ('-', 'PUNCT'), ('Ani', 'PROPN'), (',', 'PUNCT'), ('the', 'DET'), ('preacher', 'NOUN'), ('at', 'ADP'), ('the', 'DET'), ('mosque', 'NOUN'), ('in', 'ADP'), ('the', 'DET'), ('town', 'NOUN'), ('of', 'ADP'), ('Qaim', 'PROPN'), (',', 'PUNCT'), ('near', 'ADP'), ('the', 'DET'), ('Syrian', 'ADJ'), ('border', 'NOUN'), ('.', 'PUNCT')]\n"
     ]
    }
   ],
   "source": [
    "training_corpus = []\n",
    "with open(trainpath, encoding='utf-8') as f:\n",
    "    for sentence_data in parse_conllu_file(f):\n",
    "        training_corpus.append(sentence_data['pos_tags'])\n",
    "\n",
    "print(f\"Loaded {len(training_corpus)} sentences from training data\")\n",
    "print(f\"Example: {training_corpus[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd92b1e85e6086e6",
   "metadata": {},
   "source": [
    "## Train HMM Model\n",
    "\n",
    "Create and train a Hidden Markov Model using the Viterbi algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9d22616",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model trained successfully!\n",
      "Number of unique words: 9875\n",
      "Number of POS tags: 17\n",
      "POS tags: ['ADJ', 'ADP', 'ADV', 'AUX', 'CCONJ', 'DET', 'INTJ', 'NOUN', 'NUM', 'PART', 'PRON', 'PROPN', 'PUNCT', 'SCONJ', 'SYM', 'VERB', 'X']\n"
     ]
    }
   ],
   "source": [
    "model = HiddenMarkovModel(training_corpus, algorithm='viterbi')\n",
    "\n",
    "print(f\"Model trained successfully!\")\n",
    "print(f\"Number of unique words: {model.n_observations}\")\n",
    "print(f\"Number of POS tags: {model.n_hidden_states}\")\n",
    "print(f\"POS tags: {model.unique_pos}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90181fbc",
   "metadata": {},
   "source": [
    "## Load Test Data\n",
    "\n",
    "Parse the test corpus to extract sentences and POS tags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "403e9caf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 2077 sentences from test data\n",
      "Example: [('What', 'PRON'), ('if', 'SCONJ'), ('Google', 'PROPN'), ('Morphed', 'VERB'), ('Into', 'ADP'), ('GoogleOS', 'PROPN'), ('?', 'PUNCT')]\n"
     ]
    }
   ],
   "source": [
    "# Load test data\n",
    "test_corpus = []\n",
    "with open(testpath, encoding='utf-8') as f:\n",
    "    for sentence_data in parse_conllu_file(f):\n",
    "        test_corpus.append(sentence_data['pos_tags'])\n",
    "\n",
    "print(f\"Loaded {len(test_corpus)} sentences from test data\")\n",
    "print(f\"Example: {test_corpus[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c96a54b1",
   "metadata": {},
   "source": [
    "## Predict POS Tags\n",
    "\n",
    "Use the trained model to predict POS tags for test sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c03f3891",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Made predictions for 2077 sentences\n",
      "Example prediction: [('What', 'PRON'), ('if', 'SCONJ'), ('Google', 'PROPN'), ('Morphed', 'PROPN'), ('Into', 'PROPN'), ('GoogleOS', 'PROPN'), ('?', 'PUNCT')]\n",
      "Actual tags:        [('What', 'PRON'), ('if', 'SCONJ'), ('Google', 'PROPN'), ('Morphed', 'VERB'), ('Into', 'ADP'), ('GoogleOS', 'PROPN'), ('?', 'PUNCT')]\n"
     ]
    }
   ],
   "source": [
    "predictions = []\n",
    "for sentence in test_corpus:\n",
    "    words = [word for word, tag in sentence]\n",
    "    predicted_tags = model.predict(words)\n",
    "    predictions.append(list(zip(words, predicted_tags)))\n",
    "\n",
    "print(f\"Made predictions for {len(predictions)} sentences\")\n",
    "print(f\"Example prediction: {predictions[0]}\")\n",
    "print(f\"Actual tags:        {test_corpus[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce67c862",
   "metadata": {},
   "source": [
    "## Evaluate Model Performance\n",
    "\n",
    "Calculate accuracy scores using micro and macro averaging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aae559ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Micro Accuracy: 0.8949\n",
      "Macro Accuracy: 0.8339\n"
     ]
    }
   ],
   "source": [
    "# Evaluate model using score method\n",
    "\n",
    "from hmm_viterbi import micro_accuracy_score, macro_accuracy_score\n",
    "\n",
    "micro_acc = model.score(test_corpus, scorer=micro_accuracy_score)\n",
    "macro_acc = model.score(test_corpus, scorer=macro_accuracy_score)\n",
    "\n",
    "print(f\"Micro Accuracy: {micro_acc:.4f}\")\n",
    "print(f\"Macro Accuracy: {macro_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60a70078",
   "metadata": {},
   "source": [
    "## Confusion Matrix\n",
    "\n",
    "Display confusion matrix to analyze per-class performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec4b5cfc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         ADJ   ADP   ADV   AUX CCONJ   DET  INTJ  NOUN   NUM  PART  PRON PROPN PUNCT SCONJ   SYM  VERB     X\n",
      "   ADJ  1487     7    38     0     0     5     1    79    18     0     0    81     0     1     0    68     3\n",
      "   ADP     1  1888    43     0     1     0     0     6     1    47     0     4     0    24     1     8     1\n",
      "   ADV    58    54   997     0     2    12     3    11     0     3     7    17     0    14     0    13     0\n",
      "   AUX     0     0     0  1510     0     0     0     3     1     0     1     1     0     0     0    26     1\n",
      " CCONJ     0     1     0     0   729     3     0     1     0     0     0     0     0     0     0     0     2\n",
      "   DET     0     0    18     0     0  1848     2     0     0     0    25     2     0     1     1     0     0\n",
      "  INTJ     3     1     2     0     0     1    92     1     0     0     1    15     0     1     0     4     0\n",
      "  NOUN    95     4     4     4     0     2     0  3501    26     0     0   363     0     0     5   109    10\n",
      "   NUM    19     0     0     0     0     1     0    19   349     0     6   131     0     0     0     7    10\n",
      "  PART     0    10     1     8     0     0     0     0     0   624     0     0     4     0     0     0     2\n",
      "  PRON     1     1     8     0     0    15     0     0     1     0  2103     4     0    31     0     0     0\n",
      " PROPN    71     3     3     1     0     0     0   261    32     0     0  1636     0     0     1    42    25\n",
      " PUNCT     0     0     0     0     0     0     0     4     1     0     0    21  3035     0    29     0     6\n",
      " SCONJ     0    71    10     0     0     3     0     4     1    10    11     0     0   268     0     6     0\n",
      "   SYM     1     0     0     0     0     0     0     2     0     0     0     3    12     0    95     0     0\n",
      "  VERB    36     2     3   106     0     2     0   110     3     0     0    58     0     0     0  2283     2\n",
      "     X     0     1     0     0     0     0     0     9     2     0     0    16     2     0     0     1    11\n"
     ]
    }
   ],
   "source": [
    "from hmm_viterbi import confusion_matrix, display_confusion_matrix\n",
    "\n",
    "# Extract true and predicted tags\n",
    "true_tags = []\n",
    "predicted_tags = []\n",
    "for sentence in test_corpus:\n",
    "    true_tags.extend([tag for word, tag in sentence])\n",
    "    \n",
    "for sentence in predictions:\n",
    "    predicted_tags.extend([tag for word, tag in sentence])\n",
    "\n",
    "# Create confusion matrix\n",
    "cm = confusion_matrix(true_tags, predicted_tags)\n",
    "display_confusion_matrix(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b34e51f0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
